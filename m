Return-Path: <linux-block-owner@vger.kernel.org>
X-Original-To: lists+linux-block@lfdr.de
Delivered-To: lists+linux-block@lfdr.de
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.lfdr.de (Postfix) with ESMTP id E1FFCB4A22
	for <lists+linux-block@lfdr.de>; Tue, 17 Sep 2019 11:14:07 +0200 (CEST)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S1726720AbfIQJOF (ORCPT <rfc822;lists+linux-block@lfdr.de>);
        Tue, 17 Sep 2019 05:14:05 -0400
Received: from mail-wm1-f65.google.com ([209.85.128.65]:36262 "EHLO
        mail-wm1-f65.google.com" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S1726843AbfIQJOE (ORCPT
        <rfc822;linux-block@vger.kernel.org>);
        Tue, 17 Sep 2019 05:14:04 -0400
Received: by mail-wm1-f65.google.com with SMTP id t3so2189435wmj.1
        for <linux-block@vger.kernel.org>; Tue, 17 Sep 2019 02:14:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=scylladb-com.20150623.gappssmtp.com; s=20150623;
        h=from:to:cc:subject:date:message-id:mime-version
         :content-transfer-encoding;
        bh=J3q851HjNurFsQDQC3tlZxSdaojcsq3jsQ2H8LllUYY=;
        b=J2og3Dg/qod5VtnEytySsQ8YyEbwaSgVGfq5v5BN26gyXBCn0RlD22jQkf+Jw9baQb
         643ndS25feWaoLtud/m27G4C853nV0HPTo6sVPwsMbu7iT76JTFMHTlONkj1/mYqsvcR
         1S65Is3DymHewzhUFIfrV5wy2EyjUqmncS08e5uiV/3OW0gvivggxW7acmHG6o+2iZnQ
         iRrE2b698SDcj6INGpjtDc516y2lbKUCZ8UJ179GWEHOpvre9DR2/EoRxQP99znszoJJ
         B6bvQB21W0R8hGuRxbaL+gg47Fc/HqhP5zW0lpNKrai0JTo7IEQGwC2hpQUyujCKoHzq
         zvdA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20161025;
        h=x-gm-message-state:from:to:cc:subject:date:message-id:mime-version
         :content-transfer-encoding;
        bh=J3q851HjNurFsQDQC3tlZxSdaojcsq3jsQ2H8LllUYY=;
        b=L+4CrTE9I/pfHTekrEZYuCcm04y5ApzhdIZlpntkxYMlUAmNbDCR8L8jJo/UQIfB1w
         w52N3HsChYQ699PyLjqfkvk9yrZgbLRZbI+9B66szA2IYz8WzlreLmeDfcKxK92rzM30
         y5el/9qSdJlTJA6RUkm+OAUqeug4Bi2+YTtz4sMxUGEhXVs2YB92ifVHIBaWVhDPjgBu
         3JKYwhCI0hD8ZE0Te5yVyZMPT4l5qc/J0f5jQj4U2KiUvbk1XTCALfpKvpNO1KbyUnoI
         CPdg7tUx8ZIYWN2Y/zjNAFghd0v4Lm7V3aI7K3Qz5k50euepLPCCaU6dyWkHuMLjjrYH
         9nqg==
X-Gm-Message-State: APjAAAUcFo2fOt1afi0G0hczjPwh+7CLbOnPpHSs4yXkhQui25QVHBBe
        jfASv11zC+rfzdpTNzHkfUzF5y/yXDE=
X-Google-Smtp-Source: APXvYqxcQxDJL4hgNO1Obo5uli3NofxH4vnmuWQtbR0GVq3vtdmlYpXAaQGx7TfY5ZFxQoXnXb9c/g==
X-Received: by 2002:a7b:c401:: with SMTP id k1mr2555955wmi.62.1568711641291;
        Tue, 17 Sep 2019 02:14:01 -0700 (PDT)
Received: from avi.cloudius-systems.com (system.cloudius-systems.com. [199.203.229.89])
        by smtp.gmail.com with ESMTPSA id y13sm4183768wrg.8.2019.09.17.02.13.59
        (version=TLS1_3 cipher=TLS_AES_256_GCM_SHA384 bits=256/256);
        Tue, 17 Sep 2019 02:14:00 -0700 (PDT)
From:   Avi Kivity <avi@scylladb.com>
To:     Jens Axboe <axboe@kernel.dk>
Cc:     linux-kernel@vger.kernel.org, linux-block@vger.kernel.org
Subject: [PATCH v1] io_uring: reserve word at cqring tail+4 for the user
Date:   Tue, 17 Sep 2019 12:13:58 +0300
Message-Id: <20190917091358.3652-1-avi@scylladb.com>
X-Mailer: git-send-email 2.21.0
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Sender: linux-block-owner@vger.kernel.org
Precedence: bulk
List-ID: <linux-block.vger.kernel.org>
X-Mailing-List: linux-block@vger.kernel.org

In some applications, a thread waits for I/O events generated by
the kernel, and also events generated by other threads in the same
application. Typically events from other threads are passed using
in-memory queues that are not known to the kernel. As long as the
threads is active, it polls for both kernel completions and
inter-thread completions; when it is idle, it tells the other threads
to use an I/O event to wait it up (e.g. an eventfd or a pipe) and
then enters the kernel, waiting for such an event or an ordinary
I/O completion.

When such a thread goes idle, it typically spins for a while to
avoid the kernel entry/exit cost in case an event is forthcoming
shortly. While it spins it polls both I/O completions and
inter-thread queues.

The x86 instruction pair UMONITOR/UMWAIT allows waiting for a cache
line to be written to. This can be used with io_uring to wait for a
wakeup without spinning (and wasting power and slowing down the other
hyperthread). Other threads can also wake up the waiter by doing a
safe write to the tail word (which triggers the wakeup), but safe
writes are slow as they require an atomic instruction. To speed up
those wakeups, reserve a word after the tail for user writes.

A thread consuming an io_uring completion queue can then use the
following sequences:

  - while busy:
    - pick up work from the completion queue and from other threads,
      and process it

  - while idle:
    - use UMONITOR/UMWAIT to wait on completions and notifications
      from other threads for a short period
    - if no work is picked up, let other threads know you will need
      a kernel wakeup, and use io_uring_enter to wait indefinitely

Signed-off-by: Avi Kivity <avi@scylladb.com>
---
 fs/io_uring.c                 | 5 +++--
 include/uapi/linux/io_uring.h | 4 ++++
 2 files changed, 7 insertions(+), 2 deletions(-)

diff --git a/fs/io_uring.c b/fs/io_uring.c
index cfb48bd088e1..4bd7905cee1d 100644
--- a/fs/io_uring.c
+++ b/fs/io_uring.c
@@ -77,12 +77,13 @@
 
 #define IORING_MAX_ENTRIES	4096
 #define IORING_MAX_FIXED_FILES	1024
 
 struct io_uring {
-	u32 head ____cacheline_aligned_in_smp;
-	u32 tail ____cacheline_aligned_in_smp;
+	u32 head ____cacheline_aligned;
+	u32 tail ____cacheline_aligned;
+	u32 reserved_for_user; // for cq ring and UMONITOR/UMWAIT (or similar) wakeups
 };
 
 /*
  * This data is shared with the application through the mmap at offset
  * IORING_OFF_SQ_RING.
diff --git a/include/uapi/linux/io_uring.h b/include/uapi/linux/io_uring.h
index 1e1652f25cc1..1a6a826a66f3 100644
--- a/include/uapi/linux/io_uring.h
+++ b/include/uapi/linux/io_uring.h
@@ -103,10 +103,14 @@ struct io_sqring_offsets {
  */
 #define IORING_SQ_NEED_WAKEUP	(1U << 0) /* needs io_uring_enter wakeup */
 
 struct io_cqring_offsets {
 	__u32 head;
+	// tail is guaranteed to be aligned on a cache line, and to have the
+	// following __u32 free for user use. This allows using e.g.
+	// UMONITOR/UMWAIT to wait on both writes to head and writes from
+	// other threads to the following word.
 	__u32 tail;
 	__u32 ring_mask;
 	__u32 ring_entries;
 	__u32 overflow;
 	__u32 cqes;
-- 
2.21.0

